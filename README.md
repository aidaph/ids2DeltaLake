# ids2DeltaLake

This package provide the tools to create a first prototype where the events generated by an Intrusion Detection System, in this case Snort, are stored in the Hadoop storage in Delta Lake format. These techniques are deployed in a distributed and high-performance mode. This prototype aims to create a platform for the post-processing of the massive data generated by Snort in Jupyter Notebooks.

* Due to it is the first version of the prototype, this is a WIP project.

### Tools installed by this module

![Workflow of ids2DeltaLake!](/assets/images/workflow.jpg "ids2DeltaLake")

* Python script for sending the files in Unified 2 format to kafka brokers
* Kafka and Kafka Connect for collecting the alerts from Snort
* Multinode infrastructure to install a cluster of Hadoop + Yarn + Spark. This use case is deployed in a 3-node cluster.
<ol>
  <li> Distributed kafka + Spark Streaming </li>
  <li> Hadoop Datanode </li>
  <li> Jupyterhub </li>
</ol>
* Jupyterhub with Jupyter Enterprise Gateway for launching kernels using Hadoop Yarn

TODO: This module use Ansible for the deployment of the different tools

### Ansible 




### Configure Snort to collect the events

The events in Snort are generated following [the output modules of the docs](http://manual-snort-org.s3-website-us-east-1.amazonaws.com/node21.html). The fastest mode to generate the logs is the configuration of Unified 2 format. U2 is a binary format used to catch all the events that travels through the network traffic. Json is one of the most common format used to generate the alerts. This prototype is based in the result of a full capture of U2 log events and json alerts.  

The type of the log data generated by Snort is organized in events and packets. First of all, a schema is defined as below:

* **event**: impact, generator-id, protocol, dport-icode, signature-revision, classification-id, signature-id, sensor-id, impact-flag, sport-itype, priority, event-second, pad2, destination-ip, event-id, mpls-label, vlan-id, source-ip, event-microsecond, blocked

* **packet**: type, packet-second, linktype, sensor-id, packet-microsecond, event-second, length, data, event-id,

This module provide the `u2kafka.py` script which parses the records from U2 to JSON and send them to the kafka brokers. Before sending the data, it formats the records (event + packet) based on the ID field. It creates a pool of threads according to `threads` parameter and create the conection to the bootstrap `kafka` servers.

#### Usage

`python3 u2kafka.py --threads <amount_of_threads> --kafka  <comma_sepparated_list_of_servers>`.



