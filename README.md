# ids2DeltaLake

This package provide the tools to create a first prototype where the events generated by the IDS Snort are stored in the Hadoop storage using the Delta Lake format. These techniques are deployed in a distributed and high-performance mode. This prototype aims to create a platform for the post-processinf of the massive data generated by Snort.

* This is the first version of the prototype, this is a WIP project.

### Tools installed by this module

TODO: This module use Ansible for the deployment of the different tools

![Workflow of ids2DeltaLake!](/assets/images/workflow.jpg "ids2DeltaLake")

* Kafka and Kafka Connect for collecting the alerts from Snort
* Python to parse the u2 binary files to json and send to kafka brokers
* Needs a multinode infrastructure to install a cluster of Hadoop + Spark. This use case is deployed in a 3-node cluster.
<ol>
  <li> Distributed kafka + Spark Streaming </li>
  <li> Hadoop Datanode </li>
  <li> Jupyterhub </li>
</ol>

### Configure Snort to collect the events

The events in Snort are generated following [the output modules of the docs](http://manual-snort-org.s3-website-us-east-1.amazonaws.com/node21.html). The fastest mode to generate the logs is the configuration of Unified 2 format. U2 is a binary format used to catch all the events that travels through the network traffic. Json is one of the most common format used to generate the alerts. This prototype is based in the result of a full capture of U2 log events and json alerts.  

The type of the log data generated by Snort is organized in events and packets. First of all, a schema is defined as below:

* **event**: impact, generator-id, protocol, dport-icode, signature-revision, classification-id, signature-id, sensor-id, impact-flag, sport-itype, priority, event-second, pad2, destination-ip, event-id, mpls-label, vlan-id, source-ip, event-microsecond, blocked

* **packet**: type, packet-second, linktype, sensor-id, packet-microsecond, event-second, length, data, event-id,

The `u2kafka.py` module parses the records from U2 to JSON and send to the kafka brokers. Before sending the data, it formats the records (event + packet) based on the ID field. It creates a pool of threads according to `threads` parameter and create the conection to the bootstrap `kafka` servers.

#### Usage

`python3 u2kafka.py --threads <amount_of_threads> --kafka  <comma_sepparated_list_of_servers>`.



